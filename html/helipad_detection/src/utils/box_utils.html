<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.8.1" />
<title>helipad_detection.src.utils.box_utils API documentation</title>
<meta name="description" content="Utility functions for bounding box processing." />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>helipad_detection.src.utils.box_utils</code></h1>
</header>
<section id="section-intro">
<p>Utility functions for bounding box processing.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># Copyright 2019 The TensorFlow Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the &#34;License&#34;);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an &#34;AS IS&#34; BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
&#34;&#34;&#34;Utility functions for bounding box processing.&#34;&#34;&#34;

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import numpy as np
import tensorflow.compat.v1 as tf


EPSILON = 1e-8
BBOX_XFORM_CLIP = np.log(1000. / 16.)


def yxyx_to_xywh(boxes):
  &#34;&#34;&#34;Converts boxes from ymin, xmin, ymax, xmax to xmin, ymin, width, height.

  Args:
    boxes: a numpy array whose last dimension is 4 representing the coordinates
      of boxes in ymin, xmin, ymax, xmax order.

  Returns:
    boxes: a numpy array whose shape is the same as `boxes` in new format.

  Raises:
    ValueError: If the last dimension of boxes is not 4.
  &#34;&#34;&#34;
  if boxes.shape[-1] != 4:
    raise ValueError(
        &#39;boxes.shape[-1] is {:d}, but must be 4.&#39;.format(boxes.shape[-1]))

  boxes_ymin = boxes[..., 0]
  boxes_xmin = boxes[..., 1]
  boxes_width = boxes[..., 3] - boxes[..., 1]
  boxes_height = boxes[..., 2] - boxes[..., 0]
  new_boxes = np.stack(
      [boxes_xmin, boxes_ymin, boxes_width, boxes_height], axis=-1)

  return new_boxes


def jitter_boxes(boxes, noise_scale=0.025):
  &#34;&#34;&#34;Jitter the box coordinates by some noise distribution.

  Args:
    boxes: a tensor whose last dimension is 4 representing the coordinates
      of boxes in ymin, xmin, ymax, xmax order.
    noise_scale: a python float which specifies the magnitude of noise. The
      rule of thumb is to set this between (0, 0.1]. The default value is found
      to mimic the noisy detections best empirically.

  Returns:
    jittered_boxes: a tensor whose shape is the same as `boxes` representing
      the jittered boxes.

  Raises:
    ValueError: If the last dimension of boxes is not 4.
  &#34;&#34;&#34;
  if boxes.shape[-1] != 4:
    raise ValueError(
        &#39;boxes.shape[-1] is {:d}, but must be 4.&#39;.format(boxes.shape[-1]))

  with tf.name_scope(&#39;jitter_boxes&#39;):
    bbox_jitters = tf.random_normal(boxes.get_shape(), stddev=noise_scale)
    ymin = boxes[..., 0:1]
    xmin = boxes[..., 1:2]
    ymax = boxes[..., 2:3]
    xmax = boxes[..., 3:4]
    width = xmax - xmin
    height = ymax - ymin
    new_center_x = (xmin + xmax) / 2.0 + bbox_jitters[..., 0:1] * width
    new_center_y = (ymin + ymax) / 2.0 + bbox_jitters[..., 1:2] * height
    new_width = width * tf.exp(bbox_jitters[..., 2:3])
    new_height = height * tf.exp(bbox_jitters[..., 3:4])
    jittered_boxes = tf.concat([
        new_center_y - new_height * 0.5,
        new_center_x - new_width * 0.5,
        new_center_y + new_height * 0.5,
        new_center_x + new_width * 0.5], axis=-1)

    return jittered_boxes


def normalize_boxes(boxes, image_shape):
  &#34;&#34;&#34;Converts boxes to the normalized coordinates.

  Args:
    boxes: a tensor whose last dimension is 4 representing the coordinates
      of boxes in ymin, xmin, ymax, xmax order.
    image_shape: a list of two integers, a two-element vector or a tensor such
      that all but the last dimensions are `broadcastable` to `boxes`. The last
      dimension is 2, which represents [height, width].

  Returns:
    normalized_boxes: a tensor whose shape is the same as `boxes` representing
      the normalized boxes.

  Raises:
    ValueError: If the last dimension of boxes is not 4.
  &#34;&#34;&#34;
  if boxes.shape[-1] != 4:
    raise ValueError(
        &#39;boxes.shape[-1] is {:d}, but must be 4.&#39;.format(boxes.shape[-1]))

  with tf.name_scope(&#39;normalize_boxes&#39;):
    if isinstance(image_shape, list) or isinstance(image_shape, tuple):
      height, width = image_shape
    else:
      image_shape = tf.cast(image_shape, dtype=boxes.dtype)
      height = image_shape[..., 0:1]
      width = image_shape[..., 1:2]

    ymin = boxes[..., 0:1] / height
    xmin = boxes[..., 1:2] / width
    ymax = boxes[..., 2:3] / height
    xmax = boxes[..., 3:4] / width

    normalized_boxes = tf.concat([ymin, xmin, ymax, xmax], axis=-1)
    return normalized_boxes


def denormalize_boxes(boxes, image_shape):
  &#34;&#34;&#34;Converts boxes normalized by [height, width] to pixel coordinates.

  Args:
    boxes: a tensor whose last dimension is 4 representing the coordinates
      of boxes in ymin, xmin, ymax, xmax order.
    image_shape: a list of two integers, a two-element vector or a tensor such
      that all but the last dimensions are `broadcastable` to `boxes`. The last
      dimension is 2, which represents [height, width].

  Returns:
    denormalized_boxes: a tensor whose shape is the same as `boxes` representing
      the denormalized boxes.

  Raises:
    ValueError: If the last dimension of boxes is not 4.
  &#34;&#34;&#34;
  with tf.name_scope(&#39;denormalize_boxes&#39;):
    if isinstance(image_shape, list) or isinstance(image_shape, tuple):
      height, width = image_shape
    else:
      image_shape = tf.cast(image_shape, dtype=boxes.dtype)
      height, width = tf.split(image_shape, 2, axis=-1)

    ymin, xmin, ymax, xmax = tf.split(boxes, 4, axis=-1)
    ymin = ymin * height
    xmin = xmin * width
    ymax = ymax * height
    xmax = xmax * width

    denormalized_boxes = tf.concat([ymin, xmin, ymax, xmax], axis=-1)
    return denormalized_boxes


def clip_boxes(boxes, image_shape):
  &#34;&#34;&#34;Clips boxes to image boundaries.

  Args:
    boxes: a tensor whose last dimension is 4 representing the coordinates
      of boxes in ymin, xmin, ymax, xmax order.
    image_shape: a list of two integers, a two-element vector or a tensor such
      that all but the last dimensions are `broadcastable` to `boxes`. The last
      dimension is 2, which represents [height, width].

  Returns:
    clipped_boxes: a tensor whose shape is the same as `boxes` representing the
      clipped boxes.

  Raises:
    ValueError: If the last dimension of boxes is not 4.
  &#34;&#34;&#34;
  if boxes.shape[-1] != 4:
    raise ValueError(
        &#39;boxes.shape[-1] is {:d}, but must be 4.&#39;.format(boxes.shape[-1]))

  with tf.name_scope(&#39;clip_boxes&#39;):
    if isinstance(image_shape, list) or isinstance(image_shape, tuple):
      height, width = image_shape
    else:
      image_shape = tf.cast(image_shape, dtype=boxes.dtype)
      height = image_shape[..., 0:1]
      width = image_shape[..., 1:2]

    ymin = boxes[..., 0:1]
    xmin = boxes[..., 1:2]
    ymax = boxes[..., 2:3]
    xmax = boxes[..., 3:4]

    clipped_ymin = tf.maximum(tf.minimum(ymin, height - 1.0), 0.0)
    clipped_ymax = tf.maximum(tf.minimum(ymax, height - 1.0), 0.0)
    clipped_xmin = tf.maximum(tf.minimum(xmin, width - 1.0), 0.0)
    clipped_xmax = tf.maximum(tf.minimum(xmax, width - 1.0), 0.0)

    clipped_boxes = tf.concat(
        [clipped_ymin, clipped_xmin, clipped_ymax, clipped_xmax],
        axis=-1)
    return clipped_boxes


def compute_outer_boxes(boxes, image_shape, scale=1.0):
  &#34;&#34;&#34;Compute outer box encloses an object with a margin.

  Args:
    boxes: a tensor whose last dimension is 4 representing the coordinates
      of boxes in ymin, xmin, ymax, xmax order.
    image_shape: a list of two integers, a two-element vector or a tensor such
      that all but the last dimensions are `broadcastable` to `boxes`. The last
      dimension is 2, which represents [height, width].
    scale: a float number specifying the scale of output outer boxes to input
      `boxes`.

  Returns:
    outer_boxes: a tensor whose shape is the same as `boxes` representing the
      outer boxes.
  &#34;&#34;&#34;
  if scale &lt; 1.0:
    raise ValueError(
        &#39;scale is {}, but outer box scale must be greater than 1.0.&#39;.format(
            scale))
  centers_y = (boxes[..., 0] + boxes[..., 2]) / 2.0
  centers_x = (boxes[..., 1] + boxes[..., 3]) / 2.0
  box_height = (boxes[..., 2] - boxes[..., 0]) * scale
  box_width = (boxes[..., 3] - boxes[..., 1]) * scale
  outer_boxes = tf.stack([centers_y - box_height / 2.0,
                          centers_x - box_width / 2.0,
                          centers_y + box_height / 2.0,
                          centers_x + box_width / 2.0], axis=1)
  outer_boxes = clip_boxes(outer_boxes, image_shape)
  return outer_boxes


def encode_boxes(boxes, anchors, weights=None):
  &#34;&#34;&#34;Encode boxes to targets.

  Args:
    boxes: a tensor whose last dimension is 4 representing the coordinates
      of boxes in ymin, xmin, ymax, xmax order.
    anchors: a tensor whose shape is the same as, or `broadcastable` to `boxes`,
      representing the coordinates of anchors in ymin, xmin, ymax, xmax order.
    weights: None or a list of four float numbers used to scale coordinates.

  Returns:
    encoded_boxes: a tensor whose shape is the same as `boxes` representing the
      encoded box targets.

  Raises:
    ValueError: If the last dimension of boxes is not 4.
  &#34;&#34;&#34;
  if boxes.shape[-1] != 4:
    raise ValueError(
        &#39;boxes.shape[-1] is {:d}, but must be 4.&#39;.format(boxes.shape[-1]))

  with tf.name_scope(&#39;encode_boxes&#39;):
    boxes = tf.cast(boxes, dtype=anchors.dtype)
    ymin = boxes[..., 0:1]
    xmin = boxes[..., 1:2]
    ymax = boxes[..., 2:3]
    xmax = boxes[..., 3:4]
    box_h = ymax - ymin + 1.0
    box_w = xmax - xmin + 1.0
    box_yc = ymin + 0.5 * box_h
    box_xc = xmin + 0.5 * box_w

    anchor_ymin = anchors[..., 0:1]
    anchor_xmin = anchors[..., 1:2]
    anchor_ymax = anchors[..., 2:3]
    anchor_xmax = anchors[..., 3:4]
    anchor_h = anchor_ymax - anchor_ymin + 1.0
    anchor_w = anchor_xmax - anchor_xmin + 1.0
    anchor_yc = anchor_ymin + 0.5 * anchor_h
    anchor_xc = anchor_xmin + 0.5 * anchor_w

    encoded_dy = (box_yc - anchor_yc) / anchor_h
    encoded_dx = (box_xc - anchor_xc) / anchor_w
    encoded_dh = tf.log(box_h / anchor_h)
    encoded_dw = tf.log(box_w / anchor_w)
    if weights:
      encoded_dy *= weights[0]
      encoded_dx *= weights[1]
      encoded_dh *= weights[2]
      encoded_dw *= weights[3]

    encoded_boxes = tf.concat(
        [encoded_dy, encoded_dx, encoded_dh, encoded_dw],
        axis=-1)
    return encoded_boxes


def decode_boxes(encoded_boxes, anchors, weights=None):
  &#34;&#34;&#34;Decode boxes.

  Args:
    encoded_boxes: a tensor whose last dimension is 4 representing the
      coordinates of encoded boxes in ymin, xmin, ymax, xmax order.
    anchors: a tensor whose shape is the same as, or `broadcastable` to `boxes`,
      representing the coordinates of anchors in ymin, xmin, ymax, xmax order.
    weights: None or a list of four float numbers used to scale coordinates.

  Returns:
    encoded_boxes: a tensor whose shape is the same as `boxes` representing the
      decoded box targets.
  &#34;&#34;&#34;
  if encoded_boxes.shape[-1] != 4:
    raise ValueError(
        &#39;encoded_boxes.shape[-1] is {:d}, but must be 4.&#39;
        .format(encoded_boxes.shape[-1]))

  with tf.name_scope(&#39;decode_boxes&#39;):
    encoded_boxes = tf.cast(encoded_boxes, dtype=anchors.dtype)
    dy = encoded_boxes[..., 0:1]
    dx = encoded_boxes[..., 1:2]
    dh = encoded_boxes[..., 2:3]
    dw = encoded_boxes[..., 3:4]
    if weights:
      dy /= weights[0]
      dx /= weights[1]
      dh /= weights[2]
      dw /= weights[3]
    dh = tf.minimum(dh, BBOX_XFORM_CLIP)
    dw = tf.minimum(dw, BBOX_XFORM_CLIP)

    anchor_ymin = anchors[..., 0:1]
    anchor_xmin = anchors[..., 1:2]
    anchor_ymax = anchors[..., 2:3]
    anchor_xmax = anchors[..., 3:4]
    anchor_h = anchor_ymax - anchor_ymin + 1.0
    anchor_w = anchor_xmax - anchor_xmin + 1.0
    anchor_yc = anchor_ymin + 0.5 * anchor_h
    anchor_xc = anchor_xmin + 0.5 * anchor_w

    decoded_boxes_yc = dy * anchor_h + anchor_yc
    decoded_boxes_xc = dx * anchor_w + anchor_xc
    decoded_boxes_h = tf.exp(dh) * anchor_h
    decoded_boxes_w = tf.exp(dw) * anchor_w

    decoded_boxes_ymin = decoded_boxes_yc - 0.5 * decoded_boxes_h
    decoded_boxes_xmin = decoded_boxes_xc - 0.5 * decoded_boxes_w
    decoded_boxes_ymax = decoded_boxes_ymin + decoded_boxes_h - 1.0
    decoded_boxes_xmax = decoded_boxes_xmin + decoded_boxes_w - 1.0

    decoded_boxes = tf.concat(
        [decoded_boxes_ymin, decoded_boxes_xmin,
         decoded_boxes_ymax, decoded_boxes_xmax],
        axis=-1)
    return decoded_boxes


def filter_boxes(boxes, scores, image_shape, min_size_threshold):
  &#34;&#34;&#34;Filter and remove boxes that are too small or fall outside the image.

  Args:
    boxes: a tensor whose last dimension is 4 representing the
      coordinates of boxes in ymin, xmin, ymax, xmax order.
    scores: a tensor whose shape is the same as tf.shape(boxes)[:-1]
      representing the original scores of the boxes.
    image_shape: a tensor whose shape is the same as, or `broadcastable` to
      `boxes` except the last dimension, which is 2, representing
      [height, width] of the scaled image.
    min_size_threshold: a float representing the minimal box size in each
      side (w.r.t. the scaled image). Boxes whose sides are smaller than it will
      be filtered out.

  Returns:
    filtered_boxes: a tensor whose shape is the same as `boxes` but with
      the position of the filtered boxes are filled with 0.
    filtered_scores: a tensor whose shape is the same as &#39;scores&#39; but with
      the positinon of the filtered boxes filled with 0.
  &#34;&#34;&#34;
  if boxes.shape[-1] != 4:
    raise ValueError(
        &#39;boxes.shape[1] is {:d}, but must be 4.&#39;.format(boxes.shape[-1]))

  with tf.name_scope(&#39;filter_boxes&#39;):
    if isinstance(image_shape, list) or isinstance(image_shape, tuple):
      height, width = image_shape
    else:
      image_shape = tf.cast(image_shape, dtype=boxes.dtype)
      height = image_shape[..., 0]
      width = image_shape[..., 1]

    ymin = boxes[..., 0]
    xmin = boxes[..., 1]
    ymax = boxes[..., 2]
    xmax = boxes[..., 3]

    h = ymax - ymin + 1.0
    w = xmax - xmin + 1.0
    yc = ymin + 0.5 * h
    xc = xmin + 0.5 * w

    min_size = tf.cast(tf.maximum(min_size_threshold, 1.0), dtype=boxes.dtype)

    filtered_size_mask = tf.logical_and(
        tf.greater(h, min_size), tf.greater(w, min_size))
    filtered_center_mask = tf.logical_and(
        tf.logical_and(tf.greater(yc, 0.0), tf.less(yc, height)),
        tf.logical_and(tf.greater(xc, 0.0), tf.less(xc, width)))
    filtered_mask = tf.logical_and(filtered_size_mask, filtered_center_mask)

    filtered_scores = tf.where(filtered_mask, scores, tf.zeros_like(scores))
    filtered_boxes = tf.cast(
        tf.expand_dims(filtered_mask, axis=-1), dtype=boxes.dtype) * boxes

    return filtered_boxes, filtered_scores


def filter_boxes_by_scores(boxes, scores, min_score_threshold):
  &#34;&#34;&#34;Filter and remove boxes whose scores are smaller than the threshold.

  Args:
    boxes: a tensor whose last dimension is 4 representing the
      coordinates of boxes in ymin, xmin, ymax, xmax order.
    scores: a tensor whose shape is the same as tf.shape(boxes)[:-1]
      representing the original scores of the boxes.
    min_score_threshold: a float representing the minimal box score threshold.
      Boxes whose score are smaller than it will be filtered out.

  Returns:
    filtered_boxes: a tensor whose shape is the same as `boxes` but with
      the position of the filtered boxes are filled with 0.
    filtered_scores: a tensor whose shape is the same as &#39;scores&#39; but with
      the
  &#34;&#34;&#34;
  if boxes.shape[-1] != 4:
    raise ValueError(
        &#39;boxes.shape[1] is {:d}, but must be 4.&#39;.format(boxes.shape[-1]))

  with tf.name_scope(&#39;filter_boxes_by_scores&#39;):
    filtered_mask = tf.greater(scores, min_score_threshold)
    filtered_scores = tf.where(filtered_mask, scores, tf.zeros_like(scores))
    filtered_boxes = tf.cast(
        tf.expand_dims(filtered_mask, axis=-1), dtype=boxes.dtype) * boxes

    return filtered_boxes, filtered_scores


def top_k_boxes(boxes, scores, k):
  &#34;&#34;&#34;Sort and select top k boxes according to the scores.

  Args:
    boxes: a tensor of shape [batch_size, N, 4] representing the coordiante of
      the boxes. N is the number of boxes per image.
    scores: a tensor of shsape [batch_size, N] representing the socre of the
      boxes.
    k: an integer or a tensor indicating the top k number.

  Returns:
    selected_boxes: a tensor of shape [batch_size, k, 4] representing the
      selected top k box coordinates.
    selected_scores: a tensor of shape [batch_size, k] representing the selected
      top k box scores.
  &#34;&#34;&#34;
  with tf.name_scope(&#39;top_k_boxes&#39;):
    selected_scores, top_k_indices = tf.nn.top_k(scores, k=k, sorted=True)

    batch_size, _ = scores.get_shape().as_list()
    if batch_size == 1:
      selected_boxes = tf.squeeze(
          tf.gather(boxes, top_k_indices, axis=1), axis=1)
    else:
      top_k_indices_shape = tf.shape(top_k_indices)
      batch_indices = (
          tf.expand_dims(tf.range(top_k_indices_shape[0]), axis=-1) *
          tf.ones([1, top_k_indices_shape[-1]], dtype=tf.int32))
      gather_nd_indices = tf.stack([batch_indices, top_k_indices], axis=-1)
      selected_boxes = tf.gather_nd(boxes, gather_nd_indices)

    return selected_boxes, selected_scores


def bbox_overlap(boxes, gt_boxes):
  &#34;&#34;&#34;Calculates the overlap between proposal and ground truth boxes.

  Some `gt_boxes` may have been padded.  The returned `iou` tensor for these
  boxes will be -1.

  Args:
    boxes: a tensor with a shape of [batch_size, N, 4]. N is the number of
      proposals before groundtruth assignment (e.g., rpn_post_nms_topn). The
      last dimension is the pixel coordinates in [ymin, xmin, ymax, xmax] form.
    gt_boxes: a tensor with a shape of [batch_size, MAX_NUM_INSTANCES, 4]. This
      tensor might have paddings with a negative value.

  Returns:
    iou: a tensor with as a shape of [batch_size, N, MAX_NUM_INSTANCES].
  &#34;&#34;&#34;
  with tf.name_scope(&#39;bbox_overlap&#39;):
    bb_y_min, bb_x_min, bb_y_max, bb_x_max = tf.split(
        value=boxes, num_or_size_splits=4, axis=2)
    gt_y_min, gt_x_min, gt_y_max, gt_x_max = tf.split(
        value=gt_boxes, num_or_size_splits=4, axis=2)

    # Calculates the intersection area.
    i_xmin = tf.maximum(bb_x_min, tf.transpose(gt_x_min, [0, 2, 1]))
    i_xmax = tf.minimum(bb_x_max, tf.transpose(gt_x_max, [0, 2, 1]))
    i_ymin = tf.maximum(bb_y_min, tf.transpose(gt_y_min, [0, 2, 1]))
    i_ymax = tf.minimum(bb_y_max, tf.transpose(gt_y_max, [0, 2, 1]))
    i_area = tf.maximum((i_xmax - i_xmin), 0) * tf.maximum((i_ymax - i_ymin), 0)

    # Calculates the union area.
    bb_area = (bb_y_max - bb_y_min) * (bb_x_max - bb_x_min)
    gt_area = (gt_y_max - gt_y_min) * (gt_x_max - gt_x_min)
    # Adds a small epsilon to avoid divide-by-zero.
    u_area = bb_area + tf.transpose(gt_area, [0, 2, 1]) - i_area + 1e-8

    # Calculates IoU.
    iou = i_area / u_area

    # Fills -1 for IoU entries between the padded ground truth boxes.
    gt_invalid_mask = tf.less(
        tf.reduce_max(gt_boxes, axis=-1, keepdims=True), 0.0)
    padding_mask = tf.logical_or(
        tf.zeros_like(bb_x_min, dtype=tf.bool),
        tf.transpose(gt_invalid_mask, [0, 2, 1]))
    iou = tf.where(padding_mask, -tf.ones_like(iou), iou)

    return iou</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="helipad_detection.src.utils.box_utils.bbox_overlap"><code class="name flex">
<span>def <span class="ident">bbox_overlap</span></span>(<span>boxes, gt_boxes)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates the overlap between proposal and ground truth boxes.</p>
<p>Some <code>gt_boxes</code> may have been padded.
The returned <code>iou</code> tensor for these
boxes will be -1.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>boxes</code></strong></dt>
<dd>a tensor with a shape of [batch_size, N, 4]. N is the number of
proposals before groundtruth assignment (e.g., rpn_post_nms_topn). The
last dimension is the pixel coordinates in [ymin, xmin, ymax, xmax] form.</dd>
<dt><strong><code>gt_boxes</code></strong></dt>
<dd>a tensor with a shape of [batch_size, MAX_NUM_INSTANCES, 4]. This
tensor might have paddings with a negative value.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>iou</code></dt>
<dd>a tensor with as a shape of [batch_size, N, MAX_NUM_INSTANCES].</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def bbox_overlap(boxes, gt_boxes):
  &#34;&#34;&#34;Calculates the overlap between proposal and ground truth boxes.

  Some `gt_boxes` may have been padded.  The returned `iou` tensor for these
  boxes will be -1.

  Args:
    boxes: a tensor with a shape of [batch_size, N, 4]. N is the number of
      proposals before groundtruth assignment (e.g., rpn_post_nms_topn). The
      last dimension is the pixel coordinates in [ymin, xmin, ymax, xmax] form.
    gt_boxes: a tensor with a shape of [batch_size, MAX_NUM_INSTANCES, 4]. This
      tensor might have paddings with a negative value.

  Returns:
    iou: a tensor with as a shape of [batch_size, N, MAX_NUM_INSTANCES].
  &#34;&#34;&#34;
  with tf.name_scope(&#39;bbox_overlap&#39;):
    bb_y_min, bb_x_min, bb_y_max, bb_x_max = tf.split(
        value=boxes, num_or_size_splits=4, axis=2)
    gt_y_min, gt_x_min, gt_y_max, gt_x_max = tf.split(
        value=gt_boxes, num_or_size_splits=4, axis=2)

    # Calculates the intersection area.
    i_xmin = tf.maximum(bb_x_min, tf.transpose(gt_x_min, [0, 2, 1]))
    i_xmax = tf.minimum(bb_x_max, tf.transpose(gt_x_max, [0, 2, 1]))
    i_ymin = tf.maximum(bb_y_min, tf.transpose(gt_y_min, [0, 2, 1]))
    i_ymax = tf.minimum(bb_y_max, tf.transpose(gt_y_max, [0, 2, 1]))
    i_area = tf.maximum((i_xmax - i_xmin), 0) * tf.maximum((i_ymax - i_ymin), 0)

    # Calculates the union area.
    bb_area = (bb_y_max - bb_y_min) * (bb_x_max - bb_x_min)
    gt_area = (gt_y_max - gt_y_min) * (gt_x_max - gt_x_min)
    # Adds a small epsilon to avoid divide-by-zero.
    u_area = bb_area + tf.transpose(gt_area, [0, 2, 1]) - i_area + 1e-8

    # Calculates IoU.
    iou = i_area / u_area

    # Fills -1 for IoU entries between the padded ground truth boxes.
    gt_invalid_mask = tf.less(
        tf.reduce_max(gt_boxes, axis=-1, keepdims=True), 0.0)
    padding_mask = tf.logical_or(
        tf.zeros_like(bb_x_min, dtype=tf.bool),
        tf.transpose(gt_invalid_mask, [0, 2, 1]))
    iou = tf.where(padding_mask, -tf.ones_like(iou), iou)

    return iou</code></pre>
</details>
</dd>
<dt id="helipad_detection.src.utils.box_utils.clip_boxes"><code class="name flex">
<span>def <span class="ident">clip_boxes</span></span>(<span>boxes, image_shape)</span>
</code></dt>
<dd>
<div class="desc"><p>Clips boxes to image boundaries.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>boxes</code></strong></dt>
<dd>a tensor whose last dimension is 4 representing the coordinates
of boxes in ymin, xmin, ymax, xmax order.</dd>
<dt><strong><code>image_shape</code></strong></dt>
<dd>a list of two integers, a two-element vector or a tensor such
that all but the last dimensions are <code>broadcastable</code> to <code>boxes</code>. The last
dimension is 2, which represents [height, width].</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>clipped_boxes</code></dt>
<dd>a tensor whose shape is the same as <code>boxes</code> representing the
clipped boxes.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If the last dimension of boxes is not 4.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def clip_boxes(boxes, image_shape):
  &#34;&#34;&#34;Clips boxes to image boundaries.

  Args:
    boxes: a tensor whose last dimension is 4 representing the coordinates
      of boxes in ymin, xmin, ymax, xmax order.
    image_shape: a list of two integers, a two-element vector or a tensor such
      that all but the last dimensions are `broadcastable` to `boxes`. The last
      dimension is 2, which represents [height, width].

  Returns:
    clipped_boxes: a tensor whose shape is the same as `boxes` representing the
      clipped boxes.

  Raises:
    ValueError: If the last dimension of boxes is not 4.
  &#34;&#34;&#34;
  if boxes.shape[-1] != 4:
    raise ValueError(
        &#39;boxes.shape[-1] is {:d}, but must be 4.&#39;.format(boxes.shape[-1]))

  with tf.name_scope(&#39;clip_boxes&#39;):
    if isinstance(image_shape, list) or isinstance(image_shape, tuple):
      height, width = image_shape
    else:
      image_shape = tf.cast(image_shape, dtype=boxes.dtype)
      height = image_shape[..., 0:1]
      width = image_shape[..., 1:2]

    ymin = boxes[..., 0:1]
    xmin = boxes[..., 1:2]
    ymax = boxes[..., 2:3]
    xmax = boxes[..., 3:4]

    clipped_ymin = tf.maximum(tf.minimum(ymin, height - 1.0), 0.0)
    clipped_ymax = tf.maximum(tf.minimum(ymax, height - 1.0), 0.0)
    clipped_xmin = tf.maximum(tf.minimum(xmin, width - 1.0), 0.0)
    clipped_xmax = tf.maximum(tf.minimum(xmax, width - 1.0), 0.0)

    clipped_boxes = tf.concat(
        [clipped_ymin, clipped_xmin, clipped_ymax, clipped_xmax],
        axis=-1)
    return clipped_boxes</code></pre>
</details>
</dd>
<dt id="helipad_detection.src.utils.box_utils.compute_outer_boxes"><code class="name flex">
<span>def <span class="ident">compute_outer_boxes</span></span>(<span>boxes, image_shape, scale=1.0)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute outer box encloses an object with a margin.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>boxes</code></strong></dt>
<dd>a tensor whose last dimension is 4 representing the coordinates
of boxes in ymin, xmin, ymax, xmax order.</dd>
<dt><strong><code>image_shape</code></strong></dt>
<dd>a list of two integers, a two-element vector or a tensor such
that all but the last dimensions are <code>broadcastable</code> to <code>boxes</code>. The last
dimension is 2, which represents [height, width].</dd>
<dt><strong><code>scale</code></strong></dt>
<dd>a float number specifying the scale of output outer boxes to input
<code>boxes</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>outer_boxes</code></dt>
<dd>a tensor whose shape is the same as <code>boxes</code> representing the
outer boxes.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_outer_boxes(boxes, image_shape, scale=1.0):
  &#34;&#34;&#34;Compute outer box encloses an object with a margin.

  Args:
    boxes: a tensor whose last dimension is 4 representing the coordinates
      of boxes in ymin, xmin, ymax, xmax order.
    image_shape: a list of two integers, a two-element vector or a tensor such
      that all but the last dimensions are `broadcastable` to `boxes`. The last
      dimension is 2, which represents [height, width].
    scale: a float number specifying the scale of output outer boxes to input
      `boxes`.

  Returns:
    outer_boxes: a tensor whose shape is the same as `boxes` representing the
      outer boxes.
  &#34;&#34;&#34;
  if scale &lt; 1.0:
    raise ValueError(
        &#39;scale is {}, but outer box scale must be greater than 1.0.&#39;.format(
            scale))
  centers_y = (boxes[..., 0] + boxes[..., 2]) / 2.0
  centers_x = (boxes[..., 1] + boxes[..., 3]) / 2.0
  box_height = (boxes[..., 2] - boxes[..., 0]) * scale
  box_width = (boxes[..., 3] - boxes[..., 1]) * scale
  outer_boxes = tf.stack([centers_y - box_height / 2.0,
                          centers_x - box_width / 2.0,
                          centers_y + box_height / 2.0,
                          centers_x + box_width / 2.0], axis=1)
  outer_boxes = clip_boxes(outer_boxes, image_shape)
  return outer_boxes</code></pre>
</details>
</dd>
<dt id="helipad_detection.src.utils.box_utils.decode_boxes"><code class="name flex">
<span>def <span class="ident">decode_boxes</span></span>(<span>encoded_boxes, anchors, weights=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Decode boxes.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>encoded_boxes</code></strong></dt>
<dd>a tensor whose last dimension is 4 representing the
coordinates of encoded boxes in ymin, xmin, ymax, xmax order.</dd>
<dt><strong><code>anchors</code></strong></dt>
<dd>a tensor whose shape is the same as, or <code>broadcastable</code> to <code>boxes</code>,
representing the coordinates of anchors in ymin, xmin, ymax, xmax order.</dd>
<dt><strong><code>weights</code></strong></dt>
<dd>None or a list of four float numbers used to scale coordinates.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>encoded_boxes</code></dt>
<dd>a tensor whose shape is the same as <code>boxes</code> representing the
decoded box targets.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def decode_boxes(encoded_boxes, anchors, weights=None):
  &#34;&#34;&#34;Decode boxes.

  Args:
    encoded_boxes: a tensor whose last dimension is 4 representing the
      coordinates of encoded boxes in ymin, xmin, ymax, xmax order.
    anchors: a tensor whose shape is the same as, or `broadcastable` to `boxes`,
      representing the coordinates of anchors in ymin, xmin, ymax, xmax order.
    weights: None or a list of four float numbers used to scale coordinates.

  Returns:
    encoded_boxes: a tensor whose shape is the same as `boxes` representing the
      decoded box targets.
  &#34;&#34;&#34;
  if encoded_boxes.shape[-1] != 4:
    raise ValueError(
        &#39;encoded_boxes.shape[-1] is {:d}, but must be 4.&#39;
        .format(encoded_boxes.shape[-1]))

  with tf.name_scope(&#39;decode_boxes&#39;):
    encoded_boxes = tf.cast(encoded_boxes, dtype=anchors.dtype)
    dy = encoded_boxes[..., 0:1]
    dx = encoded_boxes[..., 1:2]
    dh = encoded_boxes[..., 2:3]
    dw = encoded_boxes[..., 3:4]
    if weights:
      dy /= weights[0]
      dx /= weights[1]
      dh /= weights[2]
      dw /= weights[3]
    dh = tf.minimum(dh, BBOX_XFORM_CLIP)
    dw = tf.minimum(dw, BBOX_XFORM_CLIP)

    anchor_ymin = anchors[..., 0:1]
    anchor_xmin = anchors[..., 1:2]
    anchor_ymax = anchors[..., 2:3]
    anchor_xmax = anchors[..., 3:4]
    anchor_h = anchor_ymax - anchor_ymin + 1.0
    anchor_w = anchor_xmax - anchor_xmin + 1.0
    anchor_yc = anchor_ymin + 0.5 * anchor_h
    anchor_xc = anchor_xmin + 0.5 * anchor_w

    decoded_boxes_yc = dy * anchor_h + anchor_yc
    decoded_boxes_xc = dx * anchor_w + anchor_xc
    decoded_boxes_h = tf.exp(dh) * anchor_h
    decoded_boxes_w = tf.exp(dw) * anchor_w

    decoded_boxes_ymin = decoded_boxes_yc - 0.5 * decoded_boxes_h
    decoded_boxes_xmin = decoded_boxes_xc - 0.5 * decoded_boxes_w
    decoded_boxes_ymax = decoded_boxes_ymin + decoded_boxes_h - 1.0
    decoded_boxes_xmax = decoded_boxes_xmin + decoded_boxes_w - 1.0

    decoded_boxes = tf.concat(
        [decoded_boxes_ymin, decoded_boxes_xmin,
         decoded_boxes_ymax, decoded_boxes_xmax],
        axis=-1)
    return decoded_boxes</code></pre>
</details>
</dd>
<dt id="helipad_detection.src.utils.box_utils.denormalize_boxes"><code class="name flex">
<span>def <span class="ident">denormalize_boxes</span></span>(<span>boxes, image_shape)</span>
</code></dt>
<dd>
<div class="desc"><p>Converts boxes normalized by [height, width] to pixel coordinates.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>boxes</code></strong></dt>
<dd>a tensor whose last dimension is 4 representing the coordinates
of boxes in ymin, xmin, ymax, xmax order.</dd>
<dt><strong><code>image_shape</code></strong></dt>
<dd>a list of two integers, a two-element vector or a tensor such
that all but the last dimensions are <code>broadcastable</code> to <code>boxes</code>. The last
dimension is 2, which represents [height, width].</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>denormalized_boxes</code></dt>
<dd>a tensor whose shape is the same as <code>boxes</code> representing
the denormalized boxes.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If the last dimension of boxes is not 4.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def denormalize_boxes(boxes, image_shape):
  &#34;&#34;&#34;Converts boxes normalized by [height, width] to pixel coordinates.

  Args:
    boxes: a tensor whose last dimension is 4 representing the coordinates
      of boxes in ymin, xmin, ymax, xmax order.
    image_shape: a list of two integers, a two-element vector or a tensor such
      that all but the last dimensions are `broadcastable` to `boxes`. The last
      dimension is 2, which represents [height, width].

  Returns:
    denormalized_boxes: a tensor whose shape is the same as `boxes` representing
      the denormalized boxes.

  Raises:
    ValueError: If the last dimension of boxes is not 4.
  &#34;&#34;&#34;
  with tf.name_scope(&#39;denormalize_boxes&#39;):
    if isinstance(image_shape, list) or isinstance(image_shape, tuple):
      height, width = image_shape
    else:
      image_shape = tf.cast(image_shape, dtype=boxes.dtype)
      height, width = tf.split(image_shape, 2, axis=-1)

    ymin, xmin, ymax, xmax = tf.split(boxes, 4, axis=-1)
    ymin = ymin * height
    xmin = xmin * width
    ymax = ymax * height
    xmax = xmax * width

    denormalized_boxes = tf.concat([ymin, xmin, ymax, xmax], axis=-1)
    return denormalized_boxes</code></pre>
</details>
</dd>
<dt id="helipad_detection.src.utils.box_utils.encode_boxes"><code class="name flex">
<span>def <span class="ident">encode_boxes</span></span>(<span>boxes, anchors, weights=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Encode boxes to targets.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>boxes</code></strong></dt>
<dd>a tensor whose last dimension is 4 representing the coordinates
of boxes in ymin, xmin, ymax, xmax order.</dd>
<dt><strong><code>anchors</code></strong></dt>
<dd>a tensor whose shape is the same as, or <code>broadcastable</code> to <code>boxes</code>,
representing the coordinates of anchors in ymin, xmin, ymax, xmax order.</dd>
<dt><strong><code>weights</code></strong></dt>
<dd>None or a list of four float numbers used to scale coordinates.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>encoded_boxes</code></dt>
<dd>a tensor whose shape is the same as <code>boxes</code> representing the
encoded box targets.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If the last dimension of boxes is not 4.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def encode_boxes(boxes, anchors, weights=None):
  &#34;&#34;&#34;Encode boxes to targets.

  Args:
    boxes: a tensor whose last dimension is 4 representing the coordinates
      of boxes in ymin, xmin, ymax, xmax order.
    anchors: a tensor whose shape is the same as, or `broadcastable` to `boxes`,
      representing the coordinates of anchors in ymin, xmin, ymax, xmax order.
    weights: None or a list of four float numbers used to scale coordinates.

  Returns:
    encoded_boxes: a tensor whose shape is the same as `boxes` representing the
      encoded box targets.

  Raises:
    ValueError: If the last dimension of boxes is not 4.
  &#34;&#34;&#34;
  if boxes.shape[-1] != 4:
    raise ValueError(
        &#39;boxes.shape[-1] is {:d}, but must be 4.&#39;.format(boxes.shape[-1]))

  with tf.name_scope(&#39;encode_boxes&#39;):
    boxes = tf.cast(boxes, dtype=anchors.dtype)
    ymin = boxes[..., 0:1]
    xmin = boxes[..., 1:2]
    ymax = boxes[..., 2:3]
    xmax = boxes[..., 3:4]
    box_h = ymax - ymin + 1.0
    box_w = xmax - xmin + 1.0
    box_yc = ymin + 0.5 * box_h
    box_xc = xmin + 0.5 * box_w

    anchor_ymin = anchors[..., 0:1]
    anchor_xmin = anchors[..., 1:2]
    anchor_ymax = anchors[..., 2:3]
    anchor_xmax = anchors[..., 3:4]
    anchor_h = anchor_ymax - anchor_ymin + 1.0
    anchor_w = anchor_xmax - anchor_xmin + 1.0
    anchor_yc = anchor_ymin + 0.5 * anchor_h
    anchor_xc = anchor_xmin + 0.5 * anchor_w

    encoded_dy = (box_yc - anchor_yc) / anchor_h
    encoded_dx = (box_xc - anchor_xc) / anchor_w
    encoded_dh = tf.log(box_h / anchor_h)
    encoded_dw = tf.log(box_w / anchor_w)
    if weights:
      encoded_dy *= weights[0]
      encoded_dx *= weights[1]
      encoded_dh *= weights[2]
      encoded_dw *= weights[3]

    encoded_boxes = tf.concat(
        [encoded_dy, encoded_dx, encoded_dh, encoded_dw],
        axis=-1)
    return encoded_boxes</code></pre>
</details>
</dd>
<dt id="helipad_detection.src.utils.box_utils.filter_boxes"><code class="name flex">
<span>def <span class="ident">filter_boxes</span></span>(<span>boxes, scores, image_shape, min_size_threshold)</span>
</code></dt>
<dd>
<div class="desc"><p>Filter and remove boxes that are too small or fall outside the image.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>boxes</code></strong></dt>
<dd>a tensor whose last dimension is 4 representing the
coordinates of boxes in ymin, xmin, ymax, xmax order.</dd>
<dt><strong><code>scores</code></strong></dt>
<dd>a tensor whose shape is the same as tf.shape(boxes)[:-1]
representing the original scores of the boxes.</dd>
<dt><strong><code>image_shape</code></strong></dt>
<dd>a tensor whose shape is the same as, or <code>broadcastable</code> to
<code>boxes</code> except the last dimension, which is 2, representing
[height, width] of the scaled image.</dd>
<dt><strong><code>min_size_threshold</code></strong></dt>
<dd>a float representing the minimal box size in each
side (w.r.t. the scaled image). Boxes whose sides are smaller than it will
be filtered out.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>filtered_boxes</code></dt>
<dd>a tensor whose shape is the same as <code>boxes</code> but with
the position of the filtered boxes are filled with 0.</dd>
<dt><code>filtered_scores</code></dt>
<dd>a tensor whose shape is the same as 'scores' but with
the positinon of the filtered boxes filled with 0.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def filter_boxes(boxes, scores, image_shape, min_size_threshold):
  &#34;&#34;&#34;Filter and remove boxes that are too small or fall outside the image.

  Args:
    boxes: a tensor whose last dimension is 4 representing the
      coordinates of boxes in ymin, xmin, ymax, xmax order.
    scores: a tensor whose shape is the same as tf.shape(boxes)[:-1]
      representing the original scores of the boxes.
    image_shape: a tensor whose shape is the same as, or `broadcastable` to
      `boxes` except the last dimension, which is 2, representing
      [height, width] of the scaled image.
    min_size_threshold: a float representing the minimal box size in each
      side (w.r.t. the scaled image). Boxes whose sides are smaller than it will
      be filtered out.

  Returns:
    filtered_boxes: a tensor whose shape is the same as `boxes` but with
      the position of the filtered boxes are filled with 0.
    filtered_scores: a tensor whose shape is the same as &#39;scores&#39; but with
      the positinon of the filtered boxes filled with 0.
  &#34;&#34;&#34;
  if boxes.shape[-1] != 4:
    raise ValueError(
        &#39;boxes.shape[1] is {:d}, but must be 4.&#39;.format(boxes.shape[-1]))

  with tf.name_scope(&#39;filter_boxes&#39;):
    if isinstance(image_shape, list) or isinstance(image_shape, tuple):
      height, width = image_shape
    else:
      image_shape = tf.cast(image_shape, dtype=boxes.dtype)
      height = image_shape[..., 0]
      width = image_shape[..., 1]

    ymin = boxes[..., 0]
    xmin = boxes[..., 1]
    ymax = boxes[..., 2]
    xmax = boxes[..., 3]

    h = ymax - ymin + 1.0
    w = xmax - xmin + 1.0
    yc = ymin + 0.5 * h
    xc = xmin + 0.5 * w

    min_size = tf.cast(tf.maximum(min_size_threshold, 1.0), dtype=boxes.dtype)

    filtered_size_mask = tf.logical_and(
        tf.greater(h, min_size), tf.greater(w, min_size))
    filtered_center_mask = tf.logical_and(
        tf.logical_and(tf.greater(yc, 0.0), tf.less(yc, height)),
        tf.logical_and(tf.greater(xc, 0.0), tf.less(xc, width)))
    filtered_mask = tf.logical_and(filtered_size_mask, filtered_center_mask)

    filtered_scores = tf.where(filtered_mask, scores, tf.zeros_like(scores))
    filtered_boxes = tf.cast(
        tf.expand_dims(filtered_mask, axis=-1), dtype=boxes.dtype) * boxes

    return filtered_boxes, filtered_scores</code></pre>
</details>
</dd>
<dt id="helipad_detection.src.utils.box_utils.filter_boxes_by_scores"><code class="name flex">
<span>def <span class="ident">filter_boxes_by_scores</span></span>(<span>boxes, scores, min_score_threshold)</span>
</code></dt>
<dd>
<div class="desc"><p>Filter and remove boxes whose scores are smaller than the threshold.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>boxes</code></strong></dt>
<dd>a tensor whose last dimension is 4 representing the
coordinates of boxes in ymin, xmin, ymax, xmax order.</dd>
<dt><strong><code>scores</code></strong></dt>
<dd>a tensor whose shape is the same as tf.shape(boxes)[:-1]
representing the original scores of the boxes.</dd>
<dt><strong><code>min_score_threshold</code></strong></dt>
<dd>a float representing the minimal box score threshold.
Boxes whose score are smaller than it will be filtered out.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>filtered_boxes</code></dt>
<dd>a tensor whose shape is the same as <code>boxes</code> but with
the position of the filtered boxes are filled with 0.</dd>
<dt><code>filtered_scores</code></dt>
<dd>a tensor whose shape is the same as 'scores' but with
the</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def filter_boxes_by_scores(boxes, scores, min_score_threshold):
  &#34;&#34;&#34;Filter and remove boxes whose scores are smaller than the threshold.

  Args:
    boxes: a tensor whose last dimension is 4 representing the
      coordinates of boxes in ymin, xmin, ymax, xmax order.
    scores: a tensor whose shape is the same as tf.shape(boxes)[:-1]
      representing the original scores of the boxes.
    min_score_threshold: a float representing the minimal box score threshold.
      Boxes whose score are smaller than it will be filtered out.

  Returns:
    filtered_boxes: a tensor whose shape is the same as `boxes` but with
      the position of the filtered boxes are filled with 0.
    filtered_scores: a tensor whose shape is the same as &#39;scores&#39; but with
      the
  &#34;&#34;&#34;
  if boxes.shape[-1] != 4:
    raise ValueError(
        &#39;boxes.shape[1] is {:d}, but must be 4.&#39;.format(boxes.shape[-1]))

  with tf.name_scope(&#39;filter_boxes_by_scores&#39;):
    filtered_mask = tf.greater(scores, min_score_threshold)
    filtered_scores = tf.where(filtered_mask, scores, tf.zeros_like(scores))
    filtered_boxes = tf.cast(
        tf.expand_dims(filtered_mask, axis=-1), dtype=boxes.dtype) * boxes

    return filtered_boxes, filtered_scores</code></pre>
</details>
</dd>
<dt id="helipad_detection.src.utils.box_utils.jitter_boxes"><code class="name flex">
<span>def <span class="ident">jitter_boxes</span></span>(<span>boxes, noise_scale=0.025)</span>
</code></dt>
<dd>
<div class="desc"><p>Jitter the box coordinates by some noise distribution.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>boxes</code></strong></dt>
<dd>a tensor whose last dimension is 4 representing the coordinates
of boxes in ymin, xmin, ymax, xmax order.</dd>
<dt><strong><code>noise_scale</code></strong></dt>
<dd>a python float which specifies the magnitude of noise. The
rule of thumb is to set this between (0, 0.1]. The default value is found
to mimic the noisy detections best empirically.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>jittered_boxes</code></dt>
<dd>a tensor whose shape is the same as <code>boxes</code> representing
the jittered boxes.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If the last dimension of boxes is not 4.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def jitter_boxes(boxes, noise_scale=0.025):
  &#34;&#34;&#34;Jitter the box coordinates by some noise distribution.

  Args:
    boxes: a tensor whose last dimension is 4 representing the coordinates
      of boxes in ymin, xmin, ymax, xmax order.
    noise_scale: a python float which specifies the magnitude of noise. The
      rule of thumb is to set this between (0, 0.1]. The default value is found
      to mimic the noisy detections best empirically.

  Returns:
    jittered_boxes: a tensor whose shape is the same as `boxes` representing
      the jittered boxes.

  Raises:
    ValueError: If the last dimension of boxes is not 4.
  &#34;&#34;&#34;
  if boxes.shape[-1] != 4:
    raise ValueError(
        &#39;boxes.shape[-1] is {:d}, but must be 4.&#39;.format(boxes.shape[-1]))

  with tf.name_scope(&#39;jitter_boxes&#39;):
    bbox_jitters = tf.random_normal(boxes.get_shape(), stddev=noise_scale)
    ymin = boxes[..., 0:1]
    xmin = boxes[..., 1:2]
    ymax = boxes[..., 2:3]
    xmax = boxes[..., 3:4]
    width = xmax - xmin
    height = ymax - ymin
    new_center_x = (xmin + xmax) / 2.0 + bbox_jitters[..., 0:1] * width
    new_center_y = (ymin + ymax) / 2.0 + bbox_jitters[..., 1:2] * height
    new_width = width * tf.exp(bbox_jitters[..., 2:3])
    new_height = height * tf.exp(bbox_jitters[..., 3:4])
    jittered_boxes = tf.concat([
        new_center_y - new_height * 0.5,
        new_center_x - new_width * 0.5,
        new_center_y + new_height * 0.5,
        new_center_x + new_width * 0.5], axis=-1)

    return jittered_boxes</code></pre>
</details>
</dd>
<dt id="helipad_detection.src.utils.box_utils.normalize_boxes"><code class="name flex">
<span>def <span class="ident">normalize_boxes</span></span>(<span>boxes, image_shape)</span>
</code></dt>
<dd>
<div class="desc"><p>Converts boxes to the normalized coordinates.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>boxes</code></strong></dt>
<dd>a tensor whose last dimension is 4 representing the coordinates
of boxes in ymin, xmin, ymax, xmax order.</dd>
<dt><strong><code>image_shape</code></strong></dt>
<dd>a list of two integers, a two-element vector or a tensor such
that all but the last dimensions are <code>broadcastable</code> to <code>boxes</code>. The last
dimension is 2, which represents [height, width].</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>normalized_boxes</code></dt>
<dd>a tensor whose shape is the same as <code>boxes</code> representing
the normalized boxes.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If the last dimension of boxes is not 4.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def normalize_boxes(boxes, image_shape):
  &#34;&#34;&#34;Converts boxes to the normalized coordinates.

  Args:
    boxes: a tensor whose last dimension is 4 representing the coordinates
      of boxes in ymin, xmin, ymax, xmax order.
    image_shape: a list of two integers, a two-element vector or a tensor such
      that all but the last dimensions are `broadcastable` to `boxes`. The last
      dimension is 2, which represents [height, width].

  Returns:
    normalized_boxes: a tensor whose shape is the same as `boxes` representing
      the normalized boxes.

  Raises:
    ValueError: If the last dimension of boxes is not 4.
  &#34;&#34;&#34;
  if boxes.shape[-1] != 4:
    raise ValueError(
        &#39;boxes.shape[-1] is {:d}, but must be 4.&#39;.format(boxes.shape[-1]))

  with tf.name_scope(&#39;normalize_boxes&#39;):
    if isinstance(image_shape, list) or isinstance(image_shape, tuple):
      height, width = image_shape
    else:
      image_shape = tf.cast(image_shape, dtype=boxes.dtype)
      height = image_shape[..., 0:1]
      width = image_shape[..., 1:2]

    ymin = boxes[..., 0:1] / height
    xmin = boxes[..., 1:2] / width
    ymax = boxes[..., 2:3] / height
    xmax = boxes[..., 3:4] / width

    normalized_boxes = tf.concat([ymin, xmin, ymax, xmax], axis=-1)
    return normalized_boxes</code></pre>
</details>
</dd>
<dt id="helipad_detection.src.utils.box_utils.top_k_boxes"><code class="name flex">
<span>def <span class="ident">top_k_boxes</span></span>(<span>boxes, scores, k)</span>
</code></dt>
<dd>
<div class="desc"><p>Sort and select top k boxes according to the scores.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>boxes</code></strong></dt>
<dd>a tensor of shape [batch_size, N, 4] representing the coordiante of
the boxes. N is the number of boxes per image.</dd>
<dt><strong><code>scores</code></strong></dt>
<dd>a tensor of shsape [batch_size, N] representing the socre of the
boxes.</dd>
<dt><strong><code>k</code></strong></dt>
<dd>an integer or a tensor indicating the top k number.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>selected_boxes</code></dt>
<dd>a tensor of shape [batch_size, k, 4] representing the
selected top k box coordinates.</dd>
<dt><code>selected_scores</code></dt>
<dd>a tensor of shape [batch_size, k] representing the selected
top k box scores.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def top_k_boxes(boxes, scores, k):
  &#34;&#34;&#34;Sort and select top k boxes according to the scores.

  Args:
    boxes: a tensor of shape [batch_size, N, 4] representing the coordiante of
      the boxes. N is the number of boxes per image.
    scores: a tensor of shsape [batch_size, N] representing the socre of the
      boxes.
    k: an integer or a tensor indicating the top k number.

  Returns:
    selected_boxes: a tensor of shape [batch_size, k, 4] representing the
      selected top k box coordinates.
    selected_scores: a tensor of shape [batch_size, k] representing the selected
      top k box scores.
  &#34;&#34;&#34;
  with tf.name_scope(&#39;top_k_boxes&#39;):
    selected_scores, top_k_indices = tf.nn.top_k(scores, k=k, sorted=True)

    batch_size, _ = scores.get_shape().as_list()
    if batch_size == 1:
      selected_boxes = tf.squeeze(
          tf.gather(boxes, top_k_indices, axis=1), axis=1)
    else:
      top_k_indices_shape = tf.shape(top_k_indices)
      batch_indices = (
          tf.expand_dims(tf.range(top_k_indices_shape[0]), axis=-1) *
          tf.ones([1, top_k_indices_shape[-1]], dtype=tf.int32))
      gather_nd_indices = tf.stack([batch_indices, top_k_indices], axis=-1)
      selected_boxes = tf.gather_nd(boxes, gather_nd_indices)

    return selected_boxes, selected_scores</code></pre>
</details>
</dd>
<dt id="helipad_detection.src.utils.box_utils.yxyx_to_xywh"><code class="name flex">
<span>def <span class="ident">yxyx_to_xywh</span></span>(<span>boxes)</span>
</code></dt>
<dd>
<div class="desc"><p>Converts boxes from ymin, xmin, ymax, xmax to xmin, ymin, width, height.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>boxes</code></strong></dt>
<dd>a numpy array whose last dimension is 4 representing the coordinates
of boxes in ymin, xmin, ymax, xmax order.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>boxes</code></dt>
<dd>a numpy array whose shape is the same as <code>boxes</code> in new format.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If the last dimension of boxes is not 4.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def yxyx_to_xywh(boxes):
  &#34;&#34;&#34;Converts boxes from ymin, xmin, ymax, xmax to xmin, ymin, width, height.

  Args:
    boxes: a numpy array whose last dimension is 4 representing the coordinates
      of boxes in ymin, xmin, ymax, xmax order.

  Returns:
    boxes: a numpy array whose shape is the same as `boxes` in new format.

  Raises:
    ValueError: If the last dimension of boxes is not 4.
  &#34;&#34;&#34;
  if boxes.shape[-1] != 4:
    raise ValueError(
        &#39;boxes.shape[-1] is {:d}, but must be 4.&#39;.format(boxes.shape[-1]))

  boxes_ymin = boxes[..., 0]
  boxes_xmin = boxes[..., 1]
  boxes_width = boxes[..., 3] - boxes[..., 1]
  boxes_height = boxes[..., 2] - boxes[..., 0]
  new_boxes = np.stack(
      [boxes_xmin, boxes_ymin, boxes_width, boxes_height], axis=-1)

  return new_boxes</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="helipad_detection.src.utils" href="index.html">helipad_detection.src.utils</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="helipad_detection.src.utils.box_utils.bbox_overlap" href="#helipad_detection.src.utils.box_utils.bbox_overlap">bbox_overlap</a></code></li>
<li><code><a title="helipad_detection.src.utils.box_utils.clip_boxes" href="#helipad_detection.src.utils.box_utils.clip_boxes">clip_boxes</a></code></li>
<li><code><a title="helipad_detection.src.utils.box_utils.compute_outer_boxes" href="#helipad_detection.src.utils.box_utils.compute_outer_boxes">compute_outer_boxes</a></code></li>
<li><code><a title="helipad_detection.src.utils.box_utils.decode_boxes" href="#helipad_detection.src.utils.box_utils.decode_boxes">decode_boxes</a></code></li>
<li><code><a title="helipad_detection.src.utils.box_utils.denormalize_boxes" href="#helipad_detection.src.utils.box_utils.denormalize_boxes">denormalize_boxes</a></code></li>
<li><code><a title="helipad_detection.src.utils.box_utils.encode_boxes" href="#helipad_detection.src.utils.box_utils.encode_boxes">encode_boxes</a></code></li>
<li><code><a title="helipad_detection.src.utils.box_utils.filter_boxes" href="#helipad_detection.src.utils.box_utils.filter_boxes">filter_boxes</a></code></li>
<li><code><a title="helipad_detection.src.utils.box_utils.filter_boxes_by_scores" href="#helipad_detection.src.utils.box_utils.filter_boxes_by_scores">filter_boxes_by_scores</a></code></li>
<li><code><a title="helipad_detection.src.utils.box_utils.jitter_boxes" href="#helipad_detection.src.utils.box_utils.jitter_boxes">jitter_boxes</a></code></li>
<li><code><a title="helipad_detection.src.utils.box_utils.normalize_boxes" href="#helipad_detection.src.utils.box_utils.normalize_boxes">normalize_boxes</a></code></li>
<li><code><a title="helipad_detection.src.utils.box_utils.top_k_boxes" href="#helipad_detection.src.utils.box_utils.top_k_boxes">top_k_boxes</a></code></li>
<li><code><a title="helipad_detection.src.utils.box_utils.yxyx_to_xywh" href="#helipad_detection.src.utils.box_utils.yxyx_to_xywh">yxyx_to_xywh</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.8.1</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>